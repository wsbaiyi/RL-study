{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "065945e0-5b73-4677-9e0c-8b7b929d1a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import GridWorld_v6\n",
    "import draw\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f6f80df-6dd2-460b-8b0f-8204b01654fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        self.model.add_module('layer0', nn.Linear(state_dim, hidden_layers[0]))\n",
    "        cnt = len(hidden_layers)\n",
    "        for i in range(1, cnt):\n",
    "            self.model.add_module(f'relu{i}', nn.ReLU())\n",
    "            self.model.add_module(f'layer{i}', nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "        self.model.add_module(f'relu{cnt}', nn.ReLU())\n",
    "        self.model.add_module(f'layer{cnt}', nn.Linear(hidden_layers[cnt-1], action_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        output = F.softmax(self.model(x))\n",
    "        return output\n",
    "        \n",
    "    def get_final_layer_parameters(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd97619-cb5a-47bd-bf60-e6036085e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim, env_row, env_column, learning_rate, gamma, device):\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim =state_dim\n",
    "\n",
    "        self.env_row = env_row\n",
    "        self.env_column = env_column\n",
    "        \n",
    "        # self.weight_tabular = np.full((env_row * env_column, action_dim), 1/action_dim)  #权重矩阵，输出得套softmax层\n",
    "        self.policy_net = PolicyNet(state_dim, hidden_layers, action_dim).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)  # 使用Adam优化器\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = 1\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.policy_net.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.policy_net.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    \n",
    "    def take_action(self, state):  # 根据动作概率分布随机采样 #state都是（x，y）的\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            \n",
    "            input = torch.tensor(self.to_one_hot(state[0] * self.env_column + state[1]), dtype=torch.float).to(self.device)\n",
    "            output = self.policy_net(input)\n",
    "            now_frame_probabilities  = output.detach().cpu().numpy()\n",
    "            # print(now_frame_probabilities  )\n",
    "            try:\n",
    "                \n",
    "                action = np.random.choice(self.action_dim, p=now_frame_probabilities)\n",
    "            except:\n",
    "                print(self.getProbabilities())\n",
    "            return action\n",
    "            \n",
    "    def to_one_hot(self,x):\n",
    "        one_hot = np.zeros(self.env_column * self.env_row)\n",
    "        one_hot[x] = 1\n",
    "        return one_hot\n",
    "        \n",
    "    def getProbabilities(self):\n",
    "        # 返回一个维度是state*action的概率矩阵，每一行相加和为1\n",
    "        # states = [(i, j) for i in range(self.env_row) for j in range(self.env_column) ]\n",
    "        states = [self.to_one_hot(i) for i in range(self.env_row * self.env_column)]\n",
    "        input = torch.tensor(states, dtype=torch.float).to(self.device)\n",
    "        output = self.policy_net(input).detach().cpu().numpy()\n",
    "        return output\n",
    "    \n",
    "    def update(self, transition_dict):\n",
    "        reward_list = transition_dict['rewards']\n",
    "        state_list = transition_dict['states']\n",
    "        action_list = transition_dict['actions']\n",
    "\n",
    "        G = 0\n",
    "        return_list = [[[]for j in range(self.action_dim)] for i in range(self.env_column * self.env_row)]\n",
    "        tmp = None\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in reversed(range(len(reward_list))):  # 从最后一步算起\n",
    "\n",
    "            reward = reward_list[i]\n",
    "            state = state_list[i]\n",
    "            x,y = state[0],state[1]\n",
    "            action = action_list[i]\n",
    "            \n",
    "            G = self.gamma * G + reward\n",
    "            return_list[x*self.env_column + y][action].append(G)\n",
    "            \n",
    "            input = torch.tensor(self.to_one_hot(x * self.env_column + y), dtype=torch.float).to(self.device)\n",
    "            log_prob = torch.log(self.policy_net(input)[action]) #对概率求log，这个是用来求梯度用的中间变量\n",
    "            loss = log_prob * G  #乘以G，G是当前这个时刻点到终止时的得分，如果是正的就鼓励它，如果是负的就惩罚它\n",
    "\n",
    "            loss = -loss  #梯度上升，得把符号逆置一下\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "        #求个平均，免得一次更新那么多，梯度爆炸\n",
    "        return_mean = np.array([[sum(return_list[i][j]) / len(return_list[i][j] ) if len(return_list[i][j])!=0 else 0 for j in range(self.action_dim)] for i in range(self.env_column * self.env_row)])\n",
    "        return_sum = np.array([[sum(return_list[i][j]) if len(return_list[i][j])!=0 else 0 for j in range(self.action_dim)] for i in range(self.env_column * self.env_row)])\n",
    "        self.return_score = return_sum\n",
    "\n",
    "        self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78795747-7a0f-4dcb-affd-bc50726ec098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyperparameters, img_path):\n",
    "    \n",
    "    forbiddenAreaScore = hyperparameters['forbiddenAreaScore']\n",
    "    targetAreaScore = hyperparameters['targetAreaScore']\n",
    "    hitWallScore = hyperparameters['hitWallScore']\n",
    "    moveScore = hyperparameters['moveScore']\n",
    "    start_state = hyperparameters['start_state']\n",
    "    action_space = hyperparameters['action_space']\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    num_episodes = hyperparameters['num_episodes']\n",
    "    hidden_layers = hyperparameters['hidden_layers']\n",
    "    gamma = hyperparameters['gamma']\n",
    "    device = hyperparameters['device']\n",
    "    \n",
    "    env = GridWorld_v6.GridWorld_v6(initState=start_state, moveScore=moveScore, action_space=action_space, forbiddenAreaScore=forbiddenAreaScore, score=targetAreaScore, hitWallScore = hitWallScore,\n",
    "                                    desc = [\".....\",\".##..\",\"..#..\",\".#T#.\",\".#...\"], enterForbiddenArea=False) \n",
    "    \n",
    "    # env = GridWorld_v5.GridWorld_v5(initState=start_state, moveScore=moveScore, action_space=action_space, forbiddenAreaScore=forbiddenAreaScore, score=targetAreaScore, hitWallScore = hitWallScore,\n",
    "    #                                 desc = [\"...#..T\",\n",
    "    #                                         \".....#.\"]) \n",
    "    \n",
    "    # env = GridWorld_v5.GridWorld_v5(initState=start_state, moveScore=moveScore, action_space=action_space, forbiddenAreaScore=forbiddenAreaScore, score=targetAreaScore, hitWallScore = hitWallScore,\n",
    "    #                                 desc = [\"...#...\",\n",
    "    #                                         \".....#T\"]) \n",
    "    \n",
    "    agent = REINFORCE(state_dim = env.rows * env.columns, \n",
    "              hidden_layers = hidden_layers, \n",
    "              action_dim = env.get_action_space(), \n",
    "              env_row = env.rows, \n",
    "              env_column = env.columns, \n",
    "              learning_rate = learning_rate, \n",
    "              gamma = gamma,\n",
    "              device = device)\n",
    "    if hyperparameters['use_pretrained_model'] == True:\n",
    "        agent.load_model(hyperparameters['pretrained_model'])\n",
    "    print(agent.policy_net)\n",
    "    return_list = []\n",
    "\n",
    "    epsilon = hyperparameters['begin_epsilon']\n",
    "    final_epsilon = hyperparameters['final_epsilon']\n",
    "    for i in tqdm(range(num_episodes)):# 10000\n",
    "        if(epsilon > final_epsilon) :\n",
    "            epsilon -= hyperparameters['gamma_minus_each_episodes']\n",
    "        else:\n",
    "            epsilon = final_epsilon\n",
    "        agent.epsilon = epsilon\n",
    "\n",
    "        episode_return = 0\n",
    "        transition_dict = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'next_states': [],\n",
    "            'rewards': [],\n",
    "            'dones': []\n",
    "        }\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        cnt = 0\n",
    "        while not done:\n",
    "            cnt = cnt + 1\n",
    "            action = agent.take_action(state) ########\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            transition_dict['states'].append(state)\n",
    "            transition_dict['actions'].append(action)\n",
    "            transition_dict['next_states'].append(next_state)\n",
    "            transition_dict['rewards'].append(reward)\n",
    "            transition_dict['dones'].append(done)\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "            if cnt>hyperparameters['exploring_step']:\n",
    "                break\n",
    "            \n",
    "        return_list.append(episode_return)\n",
    "        pre_frame_probabilities = agent.getProbabilities()\n",
    "        agent.update(transition_dict)\n",
    "        now_frame_probabilities = agent.getProbabilities()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            # print(pre_frame_probabilities)\n",
    "            # print(now_frame_probabilities)\n",
    "            # print(agent.return_score)\n",
    "            draw.plot_policy(pre_frame_probabilities, now_frame_probabilities, agent.return_score , transition_dict['states'], env.get_map_description(), img_path+f\"epi-{i}\")\n",
    "            agent.save_model('./models/'+f\"{i}.pth\")\n",
    "    draw.plot_policy(pre_frame_probabilities, now_frame_probabilities, agent.return_score , transition_dict['states'], env.get_map_description(), img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ca09ad-24eb-4b30-9dd1-2cab1bdc3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiments/Penalty0_Score3_State(0~24)/Penalty0_Score3_State0\n",
      "PolicyNet(\n",
      "  (model): Sequential(\n",
      "    (layer0): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (layer1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (relu2): ReLU()\n",
      "    (layer2): Linear(in_features=32, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31986\\AppData\\Local\\Temp\\ipykernel_3652\\4234279757.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(self.model(x))\n",
      "C:\\Users\\31986\\AppData\\Local\\Temp\\ipykernel_3652\\2633211175.py:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  input = torch.tensor(states, dtype=torch.float).to(self.device)\n",
      "  0%|                                                                                       | 0/200000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'draw' has no attribute 'plot_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m xlsx_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, experiment_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_path)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m utils\u001b[38;5;241m.\u001b[39mwrite_excel(hyperparameters ,img_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, bias \u001b[38;5;241m=\u001b[39m start_state, xlsx_path\u001b[38;5;241m=\u001b[39mxlsx_path, highlights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[12], line 82\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(hyperparameters, img_path)\u001b[0m\n\u001b[0;32m     76\u001b[0m     now_frame_probabilities \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mgetProbabilities()\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;66;03m# print(pre_frame_probabilities)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;66;03m# print(now_frame_probabilities)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;66;03m# print(agent.return_score)\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m         \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_policy\u001b[49m(pre_frame_probabilities, now_frame_probabilities, agent\u001b[38;5;241m.\u001b[39mreturn_score , transition_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m], env\u001b[38;5;241m.\u001b[39mget_map_description(), img_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepi-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m         agent\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m draw\u001b[38;5;241m.\u001b[39mplot_policy(pre_frame_probabilities, now_frame_probabilities, agent\u001b[38;5;241m.\u001b[39mreturn_score , transition_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m], env\u001b[38;5;241m.\u001b[39mget_map_description(), img_path)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'draw' has no attribute 'plot_policy'"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'forbiddenAreaScore': 0,  #踩陷阱和碰壁的惩罚\n",
    "    'hitWallScore': 0,\n",
    "    'targetAreaScore': 3,     #奖励\n",
    "    'moveScore': 0,            #移动惩罚\n",
    "    'action_space': 4,         \n",
    "    'learning_rate': 0.0005,    \n",
    "    'hidden_layers': [16,32],\n",
    "    'gamma': 0.99,              #折扣因子\n",
    "    'num_episodes': 200000,      #训练轮次，每次训练epsilon -= gamma_minus_each_episodes\n",
    "    'start_state': 0,\n",
    "    'exploring_step':50, ##########################\n",
    "    \n",
    "    'begin_epsilon':0,\n",
    "    'final_epsilon':0,\n",
    "    'gamma_minus_each_episodes':0.00005,\n",
    "\n",
    "    'device':torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "\n",
    "    'use_pretrained_model':False,\n",
    "    'pretrained_model':'./models/24-40000.pth'\n",
    "}\n",
    "\n",
    "start_state = hyperparameters['start_state']\n",
    "\n",
    "# 创建 images 文件夹（如果不存在）\n",
    "experiment_name = f\"Penalty{hyperparameters['forbiddenAreaScore']}_Score{hyperparameters['targetAreaScore']}_State\"\n",
    "output_dir = f\"experiments/{experiment_name}(0~24)/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# 图片文件路径\n",
    "\n",
    "img_name = f\"{experiment_name}{start_state}\"\n",
    "img_path = os.path.join(output_dir, img_name)\n",
    "xlsx_path = os.path.join(output_dir, experiment_name+\".xlsx\")\n",
    "print(img_path)\n",
    "train(hyperparameters, img_path)\n",
    "\n",
    "utils.write_excel(hyperparameters ,img_path+'.png', bias = start_state, xlsx_path=xlsx_path, highlights=['start_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6c745-2347-41cc-ac05-8d9435322750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7747b4-5634-44e0-b017-74eb6486d5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "mind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
