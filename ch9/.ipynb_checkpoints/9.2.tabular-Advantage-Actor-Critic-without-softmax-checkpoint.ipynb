{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76953fc-d66d-461f-880c-d479db8d0ca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GridWorld_v5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mGridWorld_v5\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdraw\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'GridWorld_v5'"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import GridWorld_v5\n",
    "import draw\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894bb227-fec1-4882-9379-096ed49155f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageActorCritic:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, env_row, env_column, learning_rate_critic,learning_rate_actor, gamma):\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim =state_dim\n",
    "\n",
    "        self.env_row = env_row\n",
    "        self.env_column = env_column\n",
    "        \n",
    "        self.policy_tabular = np.full((env_row * env_column, action_dim), 1/action_dim)\n",
    "        self.state_values = np.random.random(env_row * env_column)\n",
    "        \n",
    "        self.learning_rate_actor = learning_rate_actor\n",
    "        self.learning_rate_critic = learning_rate_critic\n",
    "        \n",
    "        \n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = 1\n",
    "\n",
    "    def take_action(self, state):  # 根据动作概率分布随机采样 #state都是（x，y）的\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            x, y = state[0], state[1]\n",
    "            now_frame_probabilities  = self.policy_tabular[x *self.env_column + y]\n",
    "            action = None\n",
    "            try:\n",
    "                action = np.random.choice(self.action_dim, p=now_frame_probabilities)\n",
    "            except:\n",
    "                action = np.random.choice(self.action_dim)\n",
    "            return action\n",
    "            \n",
    "    def softmax(self, logits):\n",
    "        exps = np.exp(logits - np.max(logits))  # 为了数值稳定性，从每个logit中减去最大值\n",
    "        return exps / np.sum(exps)\n",
    "        \n",
    "    def update(self, transition_dict):\n",
    "        reward_list = transition_dict['rewards']\n",
    "        state_list = transition_dict['states']\n",
    "        next_state_list = transition_dict['next_states']\n",
    "        action_list = transition_dict['actions']\n",
    "        dones_list = transition_dict['dones']\n",
    "        losses = [[[]for j in range(self.action_dim)] for i in range(self.env_column * self.env_row)]\n",
    "                \n",
    "        for i in reversed(range(len(reward_list))):  # 从最后一步算起\n",
    "            # unzip data\n",
    "            x,y = state_list[i][0], state_list[i][1]\n",
    "            nx,ny = next_state_list[i][0], next_state_list[i][1]\n",
    "            tmpstate, tmpaction, tmpscore, nextState, terminal = x*self.env_column+y ,action_list[i], reward_list[i], nx*self.env_column + ny, dones_list[i] \n",
    "            \n",
    "            # Critic update\n",
    "            # 更新state_value\n",
    "            next_state_value = self.state_values[nextState]\n",
    "            target = tmpscore + (1.0 - terminal) * self.gamma * next_state_value\n",
    "            TD_error = target - self.state_values[tmpstate]\n",
    "            self.state_values[tmpstate] += self.learning_rate_critic * TD_error\n",
    "\n",
    "            losses[tmpstate][tmpaction].append(TD_error)\n",
    "            \n",
    "            # Actor update\n",
    "            # 更新policy_tabular\n",
    "            grad = 1 / self.policy_tabular[tmpstate][tmpaction]\n",
    "            self.policy_tabular[tmpstate][tmpaction] += self.learning_rate_actor * grad * TD_error\n",
    "            #重新规范化，把概率总和变成0~1\n",
    "            # self.policy_tabular = np.maximum(self.policy_tabular, 0) #把小于0的概率规范化成0\n",
    "            self.policy_tabular = np.clip(self.policy_tabular, 0.001, 1) #把小于0的概率规范化成0\n",
    "            self.policy_tabular = self.policy_tabular/self.policy_tabular.sum(axis=1)[:,None]\n",
    "            self.policy_tabular = np.nan_to_num(self.policy_tabular, nan=1 / self.action_dim)\n",
    "            \n",
    "        loss_mean = np.array([[sum(losses[i][j]) / len(losses[i][j] ) if len(losses[i][j])!=0 else 0 for j in range(self.action_dim)] for i in range(self.env_column * self.env_row)])\n",
    "        loss_sum = np.array([[sum(losses[i][j]) if len(losses[i][j])!=0 else 0 for j in range(self.action_dim)] for i in range(self.env_column * self.env_row)])\n",
    "        loss = loss_mean\n",
    "        self.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad362e6-f5f3-4383-bef4-ee1ff6f027ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyperparameters, img_path):\n",
    "    rows = 5      #记得行数和列数这里要同步改\n",
    "    columns = 5\n",
    "    \n",
    "    forbiddenAreaScore = hyperparameters['forbiddenAreaScore']\n",
    "    targetAreaScore = hyperparameters['targetAreaScore']\n",
    "    hitWallScore = hyperparameters['hitWallScore']\n",
    "    moveScore = hyperparameters['moveScore']\n",
    "    start_state = hyperparameters['start_state']\n",
    "    action_space = hyperparameters['action_space']\n",
    "    learning_rate_critic = hyperparameters['learning_rate_critic']\n",
    "    learning_rate_actor = hyperparameters['learning_rate_actor']\n",
    "    num_episodes = hyperparameters['num_episodes']\n",
    "    hidden_dim = hyperparameters['hidden_dim']\n",
    "    gamma = hyperparameters['gamma']\n",
    "    \n",
    "    env = GridWorld_v5.GridWorld_v5(initState=start_state, moveScore=moveScore, action_space=action_space, forbiddenAreaScore=forbiddenAreaScore, score=targetAreaScore, hitWallScore = hitWallScore,\n",
    "                                    desc = [\".....\",\".##..\",\"..#..\",\".#T#.\",\".#...\"]) \n",
    "    \n",
    "    # env = GridWorld_v5.GridWorld_v5(initState=start_state, moveScore=moveScore, action_space=action_space, forbiddenAreaScore=forbiddenAreaScore, score=targetAreaScore, hitWallScore = hitWallScore,\n",
    "    #                                 desc = [\"...#..T\",\n",
    "    #                                         \".....#.\"]) \n",
    "    \n",
    "    # env = GridWorld_v5.GridWorld_v5(initState=start_state, moveScore=moveScore, action_space=action_space, forbiddenAreaScore=forbiddenAreaScore, score=targetAreaScore, hitWallScore = hitWallScore,\n",
    "    #                                 desc = [\"...#...\",\n",
    "    #                                         \".....#T\"]) \n",
    "    \n",
    "    agent = AdvantageActorCritic(state_dim = env.get_observation_space(), \n",
    "              hidden_dim = hidden_dim, \n",
    "              action_dim = env.get_action_space(), \n",
    "              env_row = env.rows, \n",
    "              env_column = env.columns, \n",
    "              learning_rate_actor = learning_rate_actor, \n",
    "              learning_rate_critic = learning_rate_critic,\n",
    "              gamma = gamma)\n",
    "    print(agent)\n",
    "    return_list = []\n",
    "\n",
    "    epsilon = 0.15\n",
    "    final_epsilon = 0.01\n",
    "    pbar = tqdm(range(num_episodes))\n",
    "    for i in pbar:# 10000\n",
    "        if(epsilon > final_epsilon) :\n",
    "            epsilon -= hyperparameters['gamma_minus_each_episodes']\n",
    "        else:\n",
    "            epsilon = final_epsilon\n",
    "        agent.epsilon = epsilon\n",
    "\n",
    "        episode_return = 0\n",
    "        transition_dict = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'next_states': [],\n",
    "            'rewards': [],\n",
    "            'dones': []\n",
    "        }\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        cnt = 0\n",
    "        while not done:\n",
    "            cnt = cnt + 1\n",
    "            action = agent.take_action(state) ########\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            transition_dict['states'].append(state)\n",
    "            transition_dict['actions'].append(action)\n",
    "            transition_dict['next_states'].append(next_state)\n",
    "            transition_dict['rewards'].append(reward)\n",
    "            transition_dict['dones'].append(done)\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "\n",
    "            # if episode_return <= forbiddenAreaScore*2:\n",
    "            #     break  #如果被惩罚了两次，那么就跳出迭代，开始扣分！！\n",
    "\n",
    "            if cnt>hyperparameters['exploring_step']:\n",
    "                break\n",
    "            \n",
    "        return_list.append(episode_return)\n",
    "        pre_frame_probabilities = agent.policy_tabular.copy()\n",
    "\n",
    "        agent.update(transition_dict)\n",
    "\n",
    "        now_frame_probabilities = agent.policy_tabular\n",
    "        pbar.set_postfix({\"epsilon\": epsilon})\n",
    "        if i % 300 == 0:\n",
    "            state_values = agent.state_values\n",
    "            p = np.argmax(agent.policy_tabular,axis=1)\n",
    "            draw.draw(state_values.reshape(5,5), p)\n",
    "            draw.plot_policy(pre_frame_probabilities, now_frame_probabilities, agent.loss , transition_dict['states'], env.get_map_description(), img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ff5b70-2690-41eb-966f-e241aab0185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiments/Penalty-100_Score50_State(0~24)/Penalty-100_Score50_State10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GridWorld_v5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m xlsx_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, experiment_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_path)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m utils\u001b[38;5;241m.\u001b[39mwrite_excel(hyperparameters ,img_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, bias \u001b[38;5;241m=\u001b[39m start_state, xlsx_path\u001b[38;5;241m=\u001b[39mxlsx_path, highlights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(hyperparameters, img_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m gamma \u001b[38;5;241m=\u001b[39m hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 17\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mGridWorld_v5\u001b[49m\u001b[38;5;241m.\u001b[39mGridWorld_v5(initState\u001b[38;5;241m=\u001b[39mstart_state, moveScore\u001b[38;5;241m=\u001b[39mmoveScore, action_space\u001b[38;5;241m=\u001b[39maction_space, forbiddenAreaScore\u001b[38;5;241m=\u001b[39mforbiddenAreaScore, score\u001b[38;5;241m=\u001b[39mtargetAreaScore, hitWallScore \u001b[38;5;241m=\u001b[39m hitWallScore,\n\u001b[0;32m     18\u001b[0m                                 desc \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.....\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.##..\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..#..\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.#T#.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.#...\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# env = GridWorld_v5.GridWorld_v5(initState=start_state, moveScore=moveScore, action_space=action_space, forbiddenAreaScore=forbiddenAreaScore, score=targetAreaScore, hitWallScore = hitWallScore,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#                                 desc = [\"...#..T\",\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#                                         \".....#.\"]) \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#                                 desc = [\"...#...\",\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#                                         \".....#T\"]) \u001b[39;00m\n\u001b[0;32m     28\u001b[0m agent \u001b[38;5;241m=\u001b[39m AdvantageActorCritic(state_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_observation_space(), \n\u001b[0;32m     29\u001b[0m           hidden_dim \u001b[38;5;241m=\u001b[39m hidden_dim, \n\u001b[0;32m     30\u001b[0m           action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_action_space(), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m           learning_rate_critic \u001b[38;5;241m=\u001b[39m learning_rate_critic,\n\u001b[0;32m     35\u001b[0m           gamma \u001b[38;5;241m=\u001b[39m gamma)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridWorld_v5' is not defined"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'forbiddenAreaScore': -100,  #踩陷阱和碰壁的惩罚\n",
    "    'hitWallScore': -10,\n",
    "    'targetAreaScore': 50,     #奖励\n",
    "    'moveScore': -1,            #移动惩罚\n",
    "    'action_space': 4,         \n",
    "    'learning_rate_critic': 0.01,    \n",
    "    'learning_rate_actor': 0.01,    \n",
    "    'hidden_dim': 512,\n",
    "    'gamma': 0.9,              #折扣因子\n",
    "    'num_episodes': 2100,      #训练轮次，每次训练epsilon -= gamma_minus_each_episodes\n",
    "    'start_state': 10,\n",
    "    'gamma_minus_each_episodes':0.00001,\n",
    "    'exploring_step':50\n",
    "}\n",
    "\n",
    "start_state = hyperparameters['start_state']\n",
    "\n",
    "# 创建 images 文件夹（如果不存在）\n",
    "experiment_name = f\"Penalty{hyperparameters['forbiddenAreaScore']}_Score{hyperparameters['targetAreaScore']}_State\"\n",
    "output_dir = f\"experiments/{experiment_name}(0~24)/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# 图片文件路径\n",
    "\n",
    "img_name = f\"{experiment_name}{start_state}\"\n",
    "img_path = os.path.join(output_dir, img_name)\n",
    "xlsx_path = os.path.join(output_dir, experiment_name+\".xlsx\")\n",
    "print(img_path)\n",
    "train(hyperparameters, img_path)\n",
    "\n",
    "utils.write_excel(hyperparameters ,img_path+'.png', bias = start_state, xlsx_path=xlsx_path, highlights=['start_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128cd6a0-326a-4fa9-9e16-8334fe639a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "mind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
