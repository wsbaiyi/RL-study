{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8886e597-63af-412f-8bbb-0bf9c8036871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from GridWorld_v2 import GridWorld_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7521e8-10a7-490e-bd40-3c6ac85716d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️🚫🚫⬜️⬜️\n",
      "⬜️⬜️🚫⬜️⬜️\n",
      "⬜️🚫✅🚫⬜️\n",
      "⬜️🚫⬜️⬜️⬜️\n"
     ]
    }
   ],
   "source": [
    "env=GridWorld_v2(forbiddenScore=-10, score=1,desc = [\".....\",\".##..\",\"..#..\",\".#T#.\",\".#...\"]) \n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e8399b-9c39-49bc-9dad-bb403465f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️⬇️⬇️➡️➡️\n",
      "🔄⏫️⏫️🔄🔄\n",
      "⬇️⬇️⏩️⬅️➡️\n",
      "⬅️⏫️⬇️🔄⬆️\n",
      "⬅️⏩️➡️⬆️➡️\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "value=np.zeros(5*5)\n",
    "q_table=np.zeros((5*5,5))\n",
    "policy=np.eye(5)[np.random.randint(0,5,size=25)]\n",
    "env.showPolicy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ffde6c5-92cd-43bd-8f3f-13597e137b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.0\n",
      "0.0\n",
      "🔄🔄🔄🔄🔄\n",
      "🔄⏫️⏩️🔄🔄\n",
      "🔄🔄⏬🔄🔄\n",
      "🔄⏪🔄⏫️🔄\n",
      "🔄⏩️🔄🔄🔄\n"
     ]
    }
   ],
   "source": [
    "pre_q_table=q_table.copy()+1\n",
    "trajectoryStep=100\n",
    "gamma=0.9\n",
    "# MC直接估计q_value\n",
    "while np.sum((pre_q_table-q_table)**2)>0.001:\n",
    "    print(np.sum((pre_q_table-q_table)**2))\n",
    "    pre_q_table=q_table.copy()\n",
    "    # 对每个(s,a)采样获得trajectory\n",
    "    for state in range(25):\n",
    "        for action in range(5):\n",
    "            \n",
    "            trajectory=env.getTrajectoryScore(trajectoryStep,state,policy,action)\n",
    "            temp=trajectory[trajectoryStep][2]\n",
    "            for i in range(trajectoryStep-1,-1,-1):\n",
    "                temp=gamma*temp+trajectory[i][2]\n",
    "            q_table[state][action]=temp\n",
    "    # policy improvement\n",
    "    policy=np.eye(5)[np.argmax(q_table,axis=1)]\n",
    "\n",
    "\n",
    "    print(np.sum((pre_q_table-q_table)**2))\n",
    "    env.showPolicy(policy)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d3b5da-e8ee-46c2-8470-422fcdb82c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄🔄🔄🔄🔄\n",
      "🔄⏫️⏩️🔄🔄\n",
      "🔄🔄⏬🔄🔄\n",
      "🔄⏪🔄⏫️🔄\n",
      "🔄⏩️🔄🔄🔄\n",
      "random policy\n",
      "125.0\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "⬆️⬆️⬆️⬆️⬆️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬆️⏫️⬆️⬆️\n",
      "⬆️⏫️⬆️⏫️⬆️\n",
      "⬆️⏫️⬆️⬆️⬆️\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#通过采样的方法计算action value，model free的话意味着不知道整个gridworld的概率了，所以不能直接套贝尔曼方程迭代求解\n",
    "env.showPolicy(policy)\n",
    "print(\"random policy\")\n",
    "trajectorySteps=100\n",
    "qtable = np.zeros((25,5))\n",
    "qtable_pre = qtable.copy()+1\n",
    "while(np.sum((qtable_pre-qtable)**2)>0.001):\n",
    "    print(np.sum((qtable_pre-qtable)**2))\n",
    "    qtable_pre = qtable.copy()\n",
    "    #通过采样获得action-value的值\n",
    "    for i in range(25):\n",
    "        for j in range(5):\n",
    "            Trajectory = env.getTrajectoryScore(trajectorySteps,state,policy,action)\n",
    "            \n",
    "            # 注意这里的返回值是大小为(trajectorySteps+1)的元组列表，因为把第一个动作也加入进去了\n",
    "            # a = r + gamma*r1 + gamma*gamma*r2 + gamma*gamma*gamma*r3 ……\n",
    "            # 返回值是 S A R\n",
    "            \n",
    "            tmp = Trajectory[trajectorySteps][2]\n",
    "            for k in range(trajectorySteps-1,-1,-1):\n",
    "                tmp = tmp*gamma + Trajectory[k][2]  #细节从后往前优化算法\n",
    "            # print(tmp)\n",
    "            \n",
    "            qtable[i][j] = tmp #这里是通过采样，获得action value的值\n",
    "\n",
    "    policy = np.eye(5)[np.argmax(qtable,axis=1)]\n",
    "    print(qtable[17])\n",
    "    print(qtable[22])\n",
    "    # policy = np.eye(5)[np.argmax(qtable,axis=1)]  #qtable的最优值作为更新策略，并用独热码来表示\n",
    "    env.showPolicy(policy)\n",
    "    print(np.sum((qtable_pre-qtable)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd4190-826b-4ab3-9fd5-f78316e59d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359a7ad-07be-48cb-954d-7a389b786154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
