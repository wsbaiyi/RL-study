{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d1e2ca5-8a12-4387-a06b-a01dddb0264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from GridWorld_v2 import GridWorld_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9acfe2f-e908-47dc-95f1-072acf632525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️🚫🚫⬜️⬜️\n",
      "⬜️⬜️🚫⬜️⬜️\n",
      "⬜️🚫✅🚫⬜️\n",
      "⬜️🚫⬜️⬜️⬜️\n"
     ]
    }
   ],
   "source": [
    "env=GridWorld_v2(forbiddenScore=-10, score=1,desc = [\".....\",\".##..\",\"..#..\",\".#T#.\",\".#...\"]) \n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "756da060-1de4-4f2d-ad03-95987ac6c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "value=np.zeros(25)\n",
    "q_table=np.zeros((25,5))\n",
    "policy=np.eye(5)[np.random.randint(0,5,size=25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a944e656-92fe-44d7-9a70-def68a4099bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️➡️➡️➡️⬅️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬇️⬅️⏫️⬆️⬇️\n",
      "🔄⏪✅⏩️🔄\n",
      "⬆️⏩️🔄⬅️⬆️\n",
      "q_table -16.524684433807227\n",
      "pre 0.0\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️➡️⬆️\n",
      "q_table -2.8\n",
      "pre -16.524684433807227\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️⬅️⬆️\n",
      "q_table -0.3226662490920818\n",
      "pre -2.8\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 0.0013140008810071748\n",
      "pre -0.3226662490920818\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 0.2928880783620802\n",
      "pre 0.0013140008810071748\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 0.5552979168609565\n",
      "pre 0.2928880783620802\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 0.7914606529886173\n",
      "pre 0.5552979168609565\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏩️➡️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 2.8995375200044453\n",
      "pre 0.7914606529886173\n",
      "➡️➡️➡️⬇️⬇️\n",
      "⬆️⏫️⏩️⬇️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 2.968615931250763\n",
      "pre 2.8995375200044453\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏩️➡️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 2.9686159321547403\n",
      "pre 2.968615931250763\n"
     ]
    }
   ],
   "source": [
    "# every visit\n",
    "pre_q_table=q_table.copy()+1\n",
    "gamma=0.9\n",
    "while np.sum((pre_q_table-q_table)**2)>0.001:\n",
    "    pre_q_table=q_table.copy()\n",
    "    trajectoryStep=100\n",
    "    q_table_reward=[[0 for j in range(5)] for i in range(25)] \n",
    "    q_table_num=[[0 for j in range(5)] for i in range(25)] \n",
    "    for state in range(25):\n",
    "        for action in range(5):\n",
    "            Trajectory=env.getTrajectoryScore(trajectoryStep,state,policy,action)\n",
    "            reward=Trajectory[trajectoryStep][2]\n",
    "            tmp=reward\n",
    "            for step in range(trajectoryStep-1,-1,-1):\n",
    "                tmp_state,tmp_action,tmp_reward,_,_=Trajectory[step]\n",
    "                tmp=tmp*gamma+tmp_reward\n",
    "                q_table_reward[tmp_state][tmp_action]+=tmp\n",
    "                q_table_num[tmp_state][tmp_action]+=1\n",
    "\n",
    "                # every visit每次到(s,a)就计算mean reward\n",
    "                q_table[tmp_state][tmp_action]=q_table_reward[tmp_state][tmp_action]/q_table_num[tmp_state][tmp_action]\n",
    "\n",
    "    # policy improvement\n",
    "    policy=np.eye(5)[np.argmax(q_table,axis=1)]\n",
    "\n",
    "    env.showPolicy(policy)\n",
    "    print('q_table',q_table.mean())    \n",
    "    print('pre',pre_q_table.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6309aff8-cda8-4bff-b366-810b54133c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️⬅️➡️⬇️🔄\n",
      "⬇️⏬⏩️⬇️⬅️\n",
      "⬆️⬅️⏬⬆️⬅️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table -19.975130819450047\n",
      "pre 0.0\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 0.2995202787849827\n",
      "pre -19.975130819450047\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 0.5619505041901922\n",
      "pre 0.2995202787849827\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 0.7981367295954016\n",
      "pre 0.5619505041901922\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏩️➡️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 2.9065749713884332\n",
      "pre 0.7981367295954016\n",
      "➡️➡️➡️⬇️⬇️\n",
      "⬆️⏫️⏩️⬇️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 2.9756410437484337\n",
      "pre 2.9065749713884332\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏩️➡️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "q_table 2.9756410437484333\n",
      "pre 2.9756410437484337\n"
     ]
    }
   ],
   "source": [
    "# first visit\n",
    "value=np.zeros(25)\n",
    "q_table=np.zeros((25,5))\n",
    "policy=np.eye(5)[np.random.randint(0,5,size=25)]\n",
    "pre_q_table=q_table.copy()+1\n",
    "gamma=0.9\n",
    "while np.sum((pre_q_table-q_table)**2)>0.001:\n",
    "    pre_q_table=q_table.copy()\n",
    "    trajectoryStep=100\n",
    "    q_table_reward=[[0 for j in range(5)] for i in range(25)] \n",
    "    q_table_num=[[0 for j in range(5)] for i in range(25)] \n",
    "    for state in range(25):\n",
    "        for action in range(5):\n",
    "            Trajectory=env.getTrajectoryScore(trajectoryStep,state,policy,action)\n",
    "            reward=Trajectory[trajectoryStep][2]\n",
    "            tmp=reward\n",
    "            for step in range(trajectoryStep-1,-1,-1):\n",
    "                tmp_state,tmp_action,tmp_reward,_,_=Trajectory[step]\n",
    "                tmp=tmp*gamma+tmp_reward\n",
    "\n",
    "                # first visit每次重新计算，只计算第一次出现的reward\n",
    "                q_table[tmp_state][tmp_action]=tmp\n",
    "\n",
    "    # policy improvement\n",
    "    policy=np.eye(5)[np.argmax(q_table,axis=1)]\n",
    "\n",
    "    env.showPolicy(policy)\n",
    "    print('q_table',q_table.mean())    \n",
    "    print('pre',pre_q_table.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217bf096-f299-4975-94e2-de835439f701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "mind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
